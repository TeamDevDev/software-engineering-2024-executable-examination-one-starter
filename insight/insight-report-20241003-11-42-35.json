{"amount_correct": 21, "percentage_score": 44, "report_time": "2024-10-03 16:42:35", "checks": [{"description": "Ensure that the README.md file exists inside of the root of the GitHub repository", "check": "ConfirmFileExists", "status": true, "path": "../README.md"}, {"description": "Delete the phrase 'Add Your Name Here' and add your own name as an Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "Add Your Name Here", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 1 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Retype every word in the Honor Code pledge in the README.md", "check": "MatchFileFragment", "options": {"fragment": "I adhered to the Allegheny College Honor Code while completing this executable examination.", "count": 3, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 2 fragment(s) in the README.md or the output while expecting exactly 3"}, {"description": "Indicate that you have completed all of the tasks in the README.md", "check": "MatchFileFragment", "options": {"fragment": "- [X]", "count": 10, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 0 fragment(s) in the README.md or the output while expecting exactly 10"}, {"description": "Use the README.md file to reference your sources in a list with each item having a suitable label", "check": "MatchFileFragment", "options": {"fragment": "- Source", "count": 1, "exact": false}, "status": false, "path": "../README.md", "diagnostic": "Found 0 fragment(s) in the README.md or the output while expecting at least 1"}, {"description": "Confirm that there are no remaining TODO markers inside of the README.md file", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 6 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Ensure that question_one.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_one.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_one.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_one.py", "diagnostic": "Found 8 fragment(s) in the question_one.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_one.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 7, "exact": true}, "status": true, "path": "questions/question_one.py"}, {"description": "Create a sufficient number of single-line comments in question_one.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_one.py"}, {"description": "Ensure that question_two.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_two.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_two.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_two.py", "diagnostic": "Found 9 fragment(s) in the question_two.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_two.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 4, "exact": true}, "status": true, "path": "questions/question_two.py"}, {"description": "Create a sufficient number of single-line comments in question_two.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_two.py"}, {"description": "Ensure that question_three.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_three.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_three.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_three.py", "diagnostic": "Found 2 fragment(s) in the question_three.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_three.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 7, "exact": true}, "status": false, "path": "questions/question_three.py", "diagnostic": "Found 5 comment(s) in the question_three.py or the output"}, {"description": "Create a sufficient number of single-line comments in question_three.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_three.py"}, {"description": "Ensure that test_question_one.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_one.py"}, {"description": "Ensure that test_question_two.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_two.py"}, {"description": "Ensure that test_question_three.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_three.py"}, {"description": "Run checks for Question 1 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_a\" --no-fancy --debug --report status --report debug --report trace", "status": false, "diagnostic": "Test Trace\n     \n     FAILED tests/test_question_one.py::test_coverage_monitoring_bubble_sort - assert 49 == 7\n     \n     test_question_one.py::test_coverage_monitoring_bubble_sort\n       - Status: Failed\n         Line: 23\n         Exact: 49 == 7\n         Message: AssertionError\n     \n     Debugging Information\n     \n     \u2714 Validity check passed for command-line arguments.\n     \u2714 Started to capture standard output and error.\n     \u2714 Correctly ran pytest when using marks.\n     \u2714 Stopped capturing standard output and error.\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_b\" --no-fancy --debug --report status --report debug --report trace", "status": false, "diagnostic": "Test Trace\n     \n     FAILED tests/test_question_one.py::test_generate_fuzzer_values - AssertionError: Character is not in range\n     \n     test_question_one.py::test_generate_fuzzer_values\n       - Status: Passed\n         Line: 31\n         Code: len(result) <= max_length\n         Exact: 9 <= 10 ...\n       - Status: Failed\n         Line: 36\n         Exact: 65 <= 48 ...\n         Message: Character is not in range\n     \n     Debugging Information\n     \n     \u2714 Validity check passed for command-line arguments.\n     \u2714 Started to capture standard output and error.\n     \u2714 Correctly ran pytest when using marks.\n     \u2714 Stopped capturing standard output and error.\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_c\" --no-fancy --debug --report status --report debug --report trace", "status": false, "diagnostic": "Test Trace\n     \n     FAILED tests/test_question_one.py::test_cgi_decode - AssertionError: Valid CGI-encoded string\n     \n     test_question_one.py::test_cgi_decode\n       - Status: Failed\n         Line: 47\n         Exact: '[Hello 0World]' == 'Hello World' ...\n         Message: Valid CGI-encoded string\n     \n     Debugging Information\n     \n     \u2714 Validity check passed for command-line arguments.\n     \u2714 Started to capture standard output and error.\n     \u2714 Correctly ran pytest when using marks.\n     \u2714 Stopped capturing standard output and error.\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_a\" --no-fancy --debug --report status --report debug --report trace", "status": false, "diagnostic": "Test Trace\n     \n     FAILED tests/test_question_two.py::test_find_minimum_value - AssertionError: Minimum positive value in dictionary\n     \n     test_question_two.py::test_find_minimum_value\n       - Status: Failed\n         Line: 15\n         Exact: (3 == None)\n         Message: Minimum positive value in dictionary\n     \n     Debugging Information\n     \n     \u2714 Validity check passed for command-line arguments.\n     \u2714 Started to capture standard output and error.\n     \u2714 Correctly ran pytest when using marks.\n     \u2714 Stopped capturing standard output and error.\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_b\" --no-fancy --debug --report status --report debug --report trace", "status": false, "diagnostic": "Test Trace\n     \n     FAILED tests/test_question_two.py::test_find_maximum_value - AssertionError: Maximum positive value in matrix\n     \n     test_question_two.py::test_find_maximum_value\n       - Status: Failed\n         Line: 42\n         Exact: 9 == None\n         Message: Maximum positive value in matrix\n     \n     Debugging Information\n     \n     \u2714 Validity check passed for command-line arguments.\n     \u2714 Started to capture standard output and error.\n     \u2714 Correctly ran pytest when using marks.\n     \u2714 Stopped capturing standard output and error.\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_c\" --no-fancy --debug --report status --report debug --report trace", "status": false, "diagnostic": "Test Trace\n     \n     FAILED tests/test_question_two.py::test_find_mode - AssertionError: Mode of an empty list should be None\n     \n     test_question_two.py::test_find_mode\n       - Status: Failed\n         Line: 61\n         Exact: 0 is None ...\n         Message: Mode of an empty list should be None\n     \n     Debugging Information\n     \n     \u2714 Validity check passed for command-line arguments.\n     \u2714 Started to capture standard output and error.\n     \u2714 Correctly ran pytest when using marks.\n     \u2714 Stopped capturing standard output and error.\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_a\" --no-fancy --debug --report status --report debug --report trace", "status": false, "diagnostic": "Test Trace\n     \n     FAILED tests/test_question_three.py::test_create_empty_lists - AssertionError: List should have one empty list when \n     count is 1\n     FAILED tests/test_question_three.py::test_count_nested_lists - AssertionError: Result should be 1 when the nested list \n     contains one empty list\n     \n     test_question_three.py::test_create_empty_lists\n       - Status: Passed\n         Line: 23\n         Code: isinstance(result, List)\n         Exact: True ...\n       - Status: Passed\n         Line: 24\n         Code: len(result) == 0\n         Exact: 0 == 0 ...\n       - Status: Passed\n         Line: 27\n         Code: isinstance(result, List)\n         Exact: True ...\n       - Status: Failed\n         Line: 28\n         Exact: 0 == 1 ...\n         Message: List should have one empty list when count is 1\n     \n     test_question_three.py::test_count_nested_lists\n       - Status: Passed\n         Line: 45\n         Code: result == 0\n         Exact: 0 == 0\n       - Status: Failed\n         Line: 48\n         Exact: 0 == 1\n         Message: Result should be 1 when the nested list contains one empty list\n     \n     Debugging Information\n     \n     \u2714 Validity check passed for command-line arguments.\n     \u2714 Started to capture standard output and error.\n     \u2714 Correctly ran pytest when using marks.\n     \u2714 Stopped capturing standard output and error.\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_b\" --no-fancy --debug --report status --report debug --report trace", "status": false, "diagnostic": "Test Trace\n     \n     FAILED tests/test_question_three.py::test_classify_triangle - AssertionError: Failed on case with all sides equal\n     \n     test_question_three.py::test_classify_triangle\n       - Status: Failed\n         Line: 76\n         Exact: 'Isosceles' == 'Equilateral' ...\n         Message: Failed on case with all sides equal\n     \n     Debugging Information\n     \n     \u2714 Validity check passed for command-line arguments.\n     \u2714 Started to capture standard output and error.\n     \u2714 Correctly ran pytest when using marks.\n     \u2714 Stopped capturing standard output and error.\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_c\" --no-fancy --debug --report status --report debug --report trace", "status": false, "diagnostic": "Test Trace\n     \n     FAILED tests/test_question_three.py::test_compute_coverage_intersection - AssertionError: Failed on case with identical \n     coverage reports\n     FAILED tests/test_question_three.py::test_compute_coverage_difference - AssertionError: Failed on case with no common \n     coverage\n     \n     test_question_three.py::test_compute_coverage_intersection\n       - Status: Failed\n         Line: 99\n         Exact: [] == [1, 2, 3] ...\n         Message: Failed on case with identical coverage reports\n     \n     test_question_three.py::test_compute_coverage_difference\n       - Status: Passed\n         Line: 124\n         Code: (\n             compute_coverage_difference([1, 2, 3], [1, 2, 3]) == []\n         )\n         Exact: [] == [] ...\n       - Status: Failed\n         Line: 127\n         Exact: [] == [1, 2, 3] ...\n         Message: Failed on case with no common coverage\n     \n     Debugging Information\n     \n     \u2714 Validity check passed for command-line arguments.\n     \u2714 Started to capture standard output and error.\n     \u2714 Correctly ran pytest when using marks.\n     \u2714 Stopped capturing standard output and error.\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Ensure that Question 1 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_one.py", "status": false, "diagnostic": "questions/question_one.py:8:1: I001 [*] Import block is un-sorted or un-formatted\n        |\n      6 |   # to the industry best practices for Python source code.\n      7 |   \n      8 | / from typing import Any, Callable, List\n      9 | | import random\n     10 | | import sys\n     11 | | \n     12 | | # Introduction: Read This First! {{{\n        | |_^ I001\n     13 |   \n     14 |   # Keep in mind these considerations as you implement the required functions:\n        |\n        = help: Organize imports\n     \n     Found 1 error.\n     [*] 1 fixable with the `--fix` option."}, {"description": "Ensure that Question 1 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_one.py --check", "status": true}, {"description": "Ensure that Question 1 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_one.py", "status": false, "diagnostic": "questions/question_one.py:234: error: Unsupported operand types for * (\"object\" and \"int\")  [operator]\n     questions/question_one.py:234: error: Unsupported operand types for + (\"int\" and \"object\")  [operator]\n     Found 2 errors in 1 file (checked 1 source file)"}, {"description": "Ensure that Question 1 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_one.py --count", "fragment": 6, "count": 1, "exact": true}, "status": true}, {"description": "Ensure that Question 1 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_one.py --count", "fragment": 6, "count": 1, "exact": true}, "status": true}, {"description": "Ensure that Question 1 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_one.py --count", "fragment": 0, "count": 1, "exact": true}, "status": true}, {"description": "Ensure that Question 2 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_two.py", "status": false, "diagnostic": "questions/question_two.py:8:1: I001 [*] Import block is un-sorted or un-formatted\n        |\n      6 |   # to the industry best practices for Python source code.\n      7 |   \n      8 | / from typing import Dict, List, Union\n      9 | | from collections import Counter\n     10 | | \n     11 | | # Introduction: Read This First! {{{\n        | |_^ I001\n     12 |   \n     13 |   # Keep in mind these considerations as you implement the required functions:\n        |\n        = help: Organize imports\n     \n     questions/question_two.py:9:25: F401 [*] `collections.Counter` imported but unused\n        |\n      8 | from typing import Dict, List, Union\n      9 | from collections import Counter\n        |                         ^^^^^^^ F401\n     10 | \n     11 | # Introduction: Read This First! {{{\n        |\n        = help: Remove unused import: `collections.Counter`\n     \n     Found 2 errors.\n     [*] 2 fixable with the `--fix` option."}, {"description": "Ensure that Question 2 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_two.py --check", "status": true}, {"description": "Ensure that Question 2 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_two.py", "status": true}, {"description": "Ensure that Question 2 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_two.py --count", "fragment": 3, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 2 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_two.py --count", "fragment": 3, "count": 1, "exact": true}, "status": true}, {"description": "Ensure that Question 2 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_two.py --count", "fragment": 0, "count": 1, "exact": true}, "status": true}, {"description": "Ensure that Question 3 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_three.py", "status": false, "diagnostic": "questions/question_three.py:8:1: I001 [*] Import block is un-sorted or un-formatted\n        |\n      6 |   # to the industry best practices for Python source code.\n      7 |   \n      8 | / from typing import List\n      9 | | from dataclasses import dataclass\n     10 | | \n     11 | | # Introduction: Read This First! {{{\n        | |_^ I001\n     12 |   \n     13 |   # Keep in mind these considerations as you implement the required functions:\n        |\n        = help: Organize imports\n     \n     questions/question_three.py:173:5: D103 Missing docstring in public function\n         |\n     173 | def compute_coverage_intersection(\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     174 |     coverage_report_one: List[int], coverage_report_two: List[int]\n     175 | ) -> List[int]:\n         |\n     \n     questions/question_three.py:182:5: D103 Missing docstring in public function\n         |\n     182 | def compute_coverage_difference(coverage_report_one, coverage_report_two):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     183 |     # initialize the coverage difference to be an empty list\n     184 |     coverage_difference = []\n         |\n     \n     Found 3 errors.\n     [*] 1 fixable with the `--fix` option."}, {"description": "Ensure that Question 3 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_three.py --check", "status": true}, {"description": "Ensure that Question 3 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_three.py", "status": false, "diagnostic": "questions/question_three.py:177: error: Need type annotation for \"coverage_intersection\" (hint: \"coverage_intersection: list[<type>] = ...\")  [var-annotated]\n     Found 1 error in 1 file (checked 1 source file)"}, {"description": "Ensure that Question 3 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_three.py --count", "fragment": 5, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 3 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_three.py --count", "fragment": 6, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 3 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_three.py --count", "fragment": 0, "count": 1, "exact": true}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}]}